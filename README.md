# LGBMRegressor
## Overview

## Resources
Here are some additional resources if you are looking to explore LightGBM more extensively:

1. [LightGBM Features](https://github.com/Microsoft/LightGBM/wiki/Features)
2. [LightGBM's Python API](https://github.com/Microsoft/LightGBM/blob/master/docs/Python-API.md)
3. [Optimisation in network communication](http://wwwi10.lrr.in.tum.de/~gerndt/home/Teaching/HPCSeminar/mpich_multi_coll.pdf)
4. [Voting parallel learning](http://papers.nips.cc/paper/6381-a-communication-efficient-parallel-algorithm-for-decision-tree)
5. [DART: Dropouts meet Multiple Additive Regression Trees](https://arxiv.org/pdf/1505.01866.pdf)
6. [Parameter configuration](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md)
7. [Fair loss](https://www.kaggle.com/c/allstate-claims-severity/discussion/24520)
8. [Poisson regression](https://en.wikipedia.org/wiki/Poisson_regression)
9. [Discounted cumulative gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG)

LightGBM is a relatively new GBM framework. If you are looking to learn more about GBMs in general, here are some additional resources about XGBoost, which has more extensive documentation and is configured very similarly to LightGBM:

1. [XGBRegressor](https://github.com/albertkklam/XGBRegressor) is a general purpose script for model training using XGBoost
2. [Introduction to Boosted Trees and the XGBoost algorithm](http://xgboost.readthedocs.io/en/latest/model.html)
3. [The Python API documentation for XGBoost](http://xgboost.readthedocs.io/en/latest/python/python_api.html)
4. [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
5. [Tong He's XGBoost presentation](https://www.slideshare.net/ShangxuanZhang/xgboost)
