{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "\n",
    "We will make extensive use of `pandas` and `LightGBM` throughout this demo. `pickle` will be used to save and load model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slack channel notifications\n",
    "\n",
    "Import `SlackClient` and create basic function that will post a Slack notification in `channel` when code is finished running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from slackclient import SlackClient\n",
    "def slack_message(message, channel):\n",
    "    token = 'your_token'\n",
    "    sc = SlackClient(token)\n",
    "    sc.api_call('chat.postMessage', channel=channel, \n",
    "                text=message, username='My Sweet Bot',\n",
    "                icon_emoji=':upside_down_face:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and set data types\n",
    "\n",
    "Set working directory and ensure schema is correct before importing train and test sets. `pd.to_datetime` automatically reads the date column `dates` - check this is correct afterwards, but it is usually pretty smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/your/directory/'  \n",
    "data_file = data_dir + 'data_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file, sep = \"\\t\", parse_dates = ['dates'], date_parser = pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine train and test set\n",
    "\n",
    "Combine `train` and `test` data sets before parsing through one-hot encoder or dense vector encoding. This is especially important for one-hot encoding because we want to maintain the same set of columns across both train and test sets. These can be inconsistent if a particular level of a categorical variable is present in one data set but not the other\n",
    "\n",
    "* `cat_cols` are categorical columns that will be used in model training\n",
    "* `index_cols` are columns that are used for indexing purposes and will not be fit in the model\n",
    "* `pred_cols` are the response variable columns\n",
    "* `num_cols` are the numeric columns that will be used in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['ATTRIBUTE_1','ATTRIBUTE_2','ATTRIBUTE_3']\n",
    "index_cols = ['FACTOR_1','FACTOR_2','FACTOR_3']\n",
    "pred_cols = ['RESPONSE']\n",
    "\n",
    "num_cols = [x for x in list(data.columns.values) if x not in cat_cols if x not in fac_cols if x not in pred_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert categorial variables to dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cat = pd.DataFrame(data[cat_cols])\n",
    "\n",
    "for feature in cat_cols: # Loop through all columns in the dataframe\n",
    "    if data_cat[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        data_cat[feature] = pd.Categorical(data[feature]).codes # Replace strings with an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare final dataframe before resplitting into train and test sets\n",
    "\n",
    "Importantly, we want to ensure that `train_final` and `test_final` are the same rows of data as `train` and `test`. `DATE_SPLIT` is the date we want to use to split our train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num = data[num_cols]\n",
    "data_final = pd.concat([data_cat, data_num], axis=1)\n",
    "data_final['DATE'] = data['DATE']\n",
    "data_final['RESPONSE'] = data['RESPONSE']\n",
    "print data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_final = data_final[data_final['DATE'] <= 'DATE_SPLIT']\n",
    "test_final = data_final[data_final['DATE'] >= 'DATE_SPLIT' ]\n",
    "\n",
    "print(train_final.shape)\n",
    "print(test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = data[data['DATE'] <= 'DATE_SPLIT']\n",
    "test = data[data['DATE'] >= 'DATE_SPLIT' ]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create design matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_final['RESPONSE']\n",
    "y_test = test_final['RESPONSE']\n",
    "x_train = train_final.drop(['RESPONSE','DATE'], axis=1)\n",
    "x_test = test_final.drop(['RESPONSE','DATE'], axis=1)\n",
    "\n",
    "print x_train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset objects for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(data = x_train, label = y_train, free_raw_data = False)\n",
    "lgb_test = lgb.Dataset(data = x_test, label = y_test, reference = lgb_train, free_raw_data = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters for LightGBM\n",
    "\n",
    "Set hyperparameters for training GBM. Early stopping rounds have also been implemented, so we can be ambitious and increase `n_estimators` to `1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth = 8\n",
    "num_leaves = 2**depth - 1\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'objective': 'regression',\n",
    "          'metric': 'l2',\n",
    "          'num_leaves': num_leaves,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.02,\n",
    "          'n_estimators': 1000,\n",
    "          'min_split_gain': 0.05,\n",
    "          'min_child_weight': 0.5,\n",
    "          'subsample': 0.8,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 0.2,\n",
    "          'reg_lambda': 0.2,\n",
    "          'seed': 100,\n",
    "          'silent': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GBM\n",
    "\n",
    "Train model against validation set. \n",
    "\n",
    "To do: Implement cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 10\n",
    "evals_result = {}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                train_set = lgb_train,\n",
    "                num_boost_round = num_boost_round,\n",
    "                valid_sets = [lgb_train, lgb_test],\n",
    "                valid_names = ['train', 'eval'],\n",
    "                evals_result = evals_result,\n",
    "                early_stopping_rounds = early_stopping_rounds,\n",
    "                verbose_eval = True\n",
    "               )\n",
    "\n",
    "slack_message(\"Booster object completed!\", 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot feature importance and print values\n",
    "\n",
    "Plot the top 30 features by `split` importance. Create dataframe that records the `split` and `gain` of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm, max_num_features = 30, importance_type='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame()\n",
    "importance['Feature'] = x_train.columns.values\n",
    "importance['ImportanceWeight'] = gbm.feature_importance(importance_type = 'split')\n",
    "importance['ImportanceGain'] = gbm.feature_importance(importance_type = 'gain')\n",
    "\n",
    "importance.sort_values(by = 'ImportanceWeight', ascending = False, inplace = True)\n",
    "importance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot L2 during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_metric(evals_result, metric='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce predictions for train and test sets before measuring accuracy\n",
    "\n",
    "Calculate predictions for both train and test sets, and then calculate MSE and RMSE for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_train_preds = gbm.predict(x_train, num_iteration = gbm.best_iteration)\n",
    "gbm_test_preds = gbm.predict(x_test, num_iteration = gbm.best_iteration)\n",
    "print gbm_train_preds.shape\n",
    "print gbm_test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"\\nModel Report\"\n",
    "print \"MSE Train : %f\" % mean_squared_error(y_train, gbm_train_preds)\n",
    "print \"MSE Test: %f\" % mean_squared_error(y_test, gbm_test_preds)\n",
    "print \"RMSE Train: %f\" % mean_squared_error(y_train, gbm_train_preds)**0.5\n",
    "print \"RMSE Test: %f\" % mean_squared_error(y_test, gbm_test_preds)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save LGBM model file and write .csv files to working directory\n",
    "\n",
    "Save LightGBM model file for future reference. Similar function to load previously saved files is commented out below. Then, write all files to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(gbm, open(\"gbm.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gbm = pickle.load(open(\"gbm.pickle.dat\", \"rb\"))\n",
    "# gbm_train_preds = gbm.predict(x_train)\n",
    "# gbm_test_preds = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print \"\\nModel Report\"\n",
    "# print \"MSE Train : %f\" % mean_squared_error(y_train, gbm_train_preds)\n",
    "# print \"MSE Test: %f\" % mean_squared_error(y_test, gbm_test_preds)\n",
    "# print \"RMSE Train: %f\" % mean_squared_error(y_train, gbm_train_preds)**0.5\n",
    "# print \"RMSE Test: %f\" % mean_squared_error(y_test, gbm_test_preds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_preds = pd.DataFrame(gbm_train_preds)\n",
    "test_preds = pd.DataFrame(gbm_test_preds)\n",
    "train_preds.columns = ['RESPONSE']\n",
    "test_preds.column = ['RESPONSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('LGBM Train.csv', sep=',')\n",
    "train_preds.to_csv('LGBM Train Preds.csv', sep=',')\n",
    "test.to_csv('LGBM Test.csv', sep=',')\n",
    "test_preds.to_csv('LGBM Test Preds.csv', sep=',')\n",
    "importance.to_csv('LGBM Feature Importance.csv', index = False)\n",
    "\n",
    "slack_message(\"Files saved!\", 'channel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
